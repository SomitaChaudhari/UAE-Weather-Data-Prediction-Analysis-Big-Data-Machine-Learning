# UAE Weather Prediction Data Analysis 

[![DOI](https://zenodo.org/badge/814232438.svg)](https://zenodo.org/doi/10.5281/zenodo.11998891) |  **Big Data & Machine Learning**   



This project focuses on analyzing weather data using Apache Spark on the Taki cluster. Various machine learning algorithms such as Linear Regression, Random Forest and Decision Trees are applied to analyze the data and make predictions.

## Setup

To replicate the analysis, follow these steps:

1. SSH into the Taki cluster using the following command:
    
    ssh username@taki.rs.umbc.edu
    

2. Run the Spark jobs using the following commands:
    
    srun --mem 5120 --partition=high_mem -N2 --time 120 --pty --account=is789sp24 $SHELL

     /path/to/spark/bin/spark-submit Linearregression.py /path/to/project/csv /path/to/project/outputfiles

    /path/to/spark/bin/spark-submit Randomforest.py /path/to/project/csv /path/to/project/outputfiles

    /path/to/spark/bin/spark-submit Decisiontree.py /path/to/project/csv /path/to/project/outputfiles


4. For Structured Streaming open a new terminal window and run the following command to start the streaming script:
    
   1st terminal:python /path/to/stream.py /path/to/project/csv /path/to/project/outputfiles

   2nd terminal :/path/to/spark/bin/spark-submit Maximumtemperature_with_streaming.py /path/to/project/csv /path/to/project/outputfiles


    

## Project Structure

- Randomforest.py: Python script implementing Random Forest algorithm for weather data analysis.
- Decisiontree.py: Python script implementing Decision Trees algorithm for weather data analysis.
- Maximumtemperature_with_streaming.py: Python script for streaming analysis of maximum temperatures.
- stream.py: Python script that periodically keeps adding csv files to the path.
- /path/to/project/csv: Directory containing input CSV files.
- /path/to/project/outputfiles: Directory containing output files generated by the Spark jobs.

## Additional Notes

- Make sure you have access to the Taki cluster and necessary permissions to run Spark jobs.
- Adjust the paths in the commands according to your directory structure.
- The streaming script (`stream.py`) should be run in a separate terminal window to continuously copy new CSV files to the input directory for structured streaming.


